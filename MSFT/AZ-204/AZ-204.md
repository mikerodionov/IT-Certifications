# Exam AZ-204: Developing Solutions for Microsoft Azure

5 OD

| OD                                                             | Percentage |
|----------------------------------------------------------------|------------|
| Develop Azure compute solutions                                | 25-30%     |
| Develop for Azure storage                                      | 15-20%     |
| Implement Azure security                                       | 20-25%     |
| Monitor, troubleshoot, and optimize Azure solutions            | 15-20%     |
| Connect to and consume Azure services and third-party services | 15-20%     |

Exam is focused on designing, building, testing and maintaining Azure solutions

## Develop Azure compute solutions - 25-30%

Various options for hosting of your workloads in the cloud.

### Azure Compute Solutions

#### Choosing Services - Decision Factors

- Price
- Scalability
- Reliability
- Infrastructure responsibility level (from IaaS to Serverless)

#### IaaS - Infrastructure as a Service

**Infrastructure as a Service (IaaS)** allows you to provision, manage, and use cloud-based infrastructure.
    - HW is managed for you, you manage infra (VMs and so on)
    - Pay for run time - idle run included

- **VMs** - classic IaaS solution (compute resource)
  - Provisioning options
    - Hardware
    - Size - HW specs/price
    - Communication
    - Resiliency - no redundancy, AZ, Availability Set
    - Image = OS
    - Disks
      - Managed (recommended) - underlying storage, availability, security and scaling - ALL IS MANAGED FOR YOU
      - Unmanaged - EVERYTHING IS MANAGED BY YOU
    - Configuration for remote access
  - Provisioning from portal/from CLI
- **Azure Container Services** - except of AKS (not in scope for AZ-204, was in scope for retired exam AZ-203)
  - **Docker** - to create container images which then used to create container instance(s)
  - **Azure Contianer Registry (ACR)** - private registry for storage and management of the images
  - **Azure Container Instance (ACI)** -simplest way to run an image without any infra management
  - With the help of the above 3 tools we can work with containers and images:
            - Create
            - Store
            - Manage
            - Deploy
            - Run

#### PaaS

- **Azure App Service** - a fully managed platform for building, deploying, and scaling your **web apps**. Web apps hosting option (similar to AWS Elastic Beanstalk) & one of the oldest/longest supported Azure services which serves over 40 billion requests every day
  - Create/monitor/manage
    - Publish - Code/Docker container
    - Publish stack
    - Runtime stack - Linux/Windows
  - Deployment - Linux/Windows/Bring Your Own Code/Containers
  - Scaling
    - App service plan - determines location, features, cost and compute resources available to your app
      - SKU (Free, Shared, Basic, Standard, Premium, Isolated) & size
        - Free - try for free
        - Shared - shared env for dev/test
        - Basic - dedicated env for dev/test
        - Standard - env for production workloads
        - Premium - enchanced performance and scale
        - Isolated - high-perfromance, security & isolation
  - App insights - for logging and monitoring

**KNOW FOR EXAM**:

- New web app set up
- How to setup a new job inside a web app (with C# SDK)
- Monitoring using app inights
- Application settings
- Setting up new WebJobs
- Deploying code
- Autoscaling rules implementation (on schedule or triggered by metrics)

#### Serverless - host your workloads without managing any servers or infra, you only define work/provide code

- **Pay for compute time** - only code execution time = lower costs if compared with IaaS/PaaS
- No infrastructure management
- Focus on workload design

**Azure Functions** - an event-based offering for carrying out serverless computations (main Azure serverless option)

- Event based solution with a range of triggers
- Implementation
- Triggers
  - Timers
  - Data Operations (something added to a Storage Queue or Blob Storage)
  - Webhooks

```C#
// Function trigger example with schedule set to every 5 minutes
[FunctionName("TimetTriggerExample")]
public static void Run(
    // Schedule syntax: Second-Minute-Hour-Day-Month-Day of the Week - 6 values
    [TimerTrigger("O */5 * * * *")]TimerInfo myTimer,
    ILogger log)
{
    log.LogInformation($"Timer trigger function executed at: {DateTime.Now}");
}
```

- Input and Output Bindings - to connect function with other services (storage blobs, CosmosDB, Event Grid, etc.) - blob and queue storage are most frequently used

```C#
// Function binding example
[FunctionName("BindingExample")]
[return: Queue("queueOutput")]
public static void Run(
[TimerTrigger("0 */5 * * * *")]TimerInfo myTimer,
[Blob("data", FileAccess.Read)] Stream input,
ILogger log)
{
log.LogInormation($"C# Timer trigger function executed at: {DateTime.Now}");
using StreamReader reader = new StreamReader(input);
return reader.ReadToEnd();
}
```

- **Durable functions** - used for long-running stateful orchestration of async operations
- From durable function we call asynchronous activity functions

## Develop for Azure storage - 15-20%

Where to store inputs/outputs for workloads you support/run. Deciding factors:

- Price
- Scalability
- Reliability
- Query type (streaming, analytics)
- Data type (structured, unstructured)

**AZ-204 scope includes ONLY Cosmos DB and Azure Blob storage types.**

### Cosmos DB

**Cosmos DB** - fully-managed NoSQL data storage solution.

- NoSQL = Not-Only SQL
  - type of databases which are used for storing nonrelational data (flexible schema and other than tabular data store format)
  - high throughput (as there is no/or lesser need for engine to apply locks to ensure consistency)
- CosmosDB pros
  - low latency
  - high throughput
  - global consistency
- CosmosDB cons
  - Cost - very high for high volumes of data, partially alleviated by Serverless pricing (not in scope for AZ-204)

- Cosmos DB account includes **3 levels** (creation order):
  - Databases (multiple per account, top level management point for containers/collection of containers)
  - Containers (multiple per database, collection of items) - partitioning and scaling are implemented on this level
  - Items (actual data/documents stored within containers)

- Partitioning implemented on container level
  - Identify values to be used as **unique IDs**
  - Identify **partition key** candidates based on scenario

- CosmosDB APIs
  - **Core (SQL) API** - query documents using SQL-like syntax
  - **Gremlin (Graph) API** - for interconnected documents with a graph-like structure
  - **Storage-like APIs**
    - MongoDB
    - Cassandra
    - Table storage
  - Exam required knowledge:
    - Choose the correct API
    - Interact with data using the SDK

- Server-Side Operations
  - Stored procedures
  - Triggers
  - Change feed notifications (full log of changes within DB)

- Consistently levels
  - **Strong** - reads guaranteed to return the most recent version
  - **Bounded staleness** - reads may lag behind by defined number of versions
  - **Session** - reads guaranteed to be consistent within a client session
  - **Consistent prefix** - guarantees that reads never see out-of-order writes (The consistent prefix model is like the bounded staleness except without the operational or time lag guarantee. It guarantees the consistency and order of the writes. However, data is not always current. For example, if data is written in A, B, C order, the user may get A, B or A, B, C. It will not get out of order writes such as A, C or A, C, B.)
  - **Eventual** - no guarantee for reads

- Define consistency baseed on requirements
  - Everyone sees the same data at all times = STRONG consistency
  - Eventually everyone sees the same data / lowest costs = EVENTUAL consistency
- Querying (using stored procedures, triggers and change feed notifications)
- Data stored as binary BLOBs and can be organized into containers (but it is stored in a flat structure)
- Use cases: static content, content delivery, logs, back ups

### Azure Blob Storage

- Part of Azure Storage offering (Table, Blob, Queue, Files, Disks)
- Storage optimized for storing large amounts of unstructured data
- Cost effective
- Large data volumes
- Store unstructured data

- Limits - see [Scalability and performance targets for standard storage accounts](https://docs.microsoft.com/en-us/azure/storage/common/scalability-targets-standard-account)
  - Default maximum storage account capacity - 5 PiB (1,024 TiB)

``
A pebibyte (PiB) is a unit of measure used to describe data capacity. The prefix pebi was created as part of the binary system for measuring computing and storage capacity, which is based on powers of two. One pebibyte equals 250 or 1,125,899,906,842,624 bytes.
``

- Blobs organized into containers
- Within containers blobs are stored in a flat structure, but folder structure can be imitated via file names ("structured blob name", e.g. "/data/report/file.txt")

#### Required knowledge

- Data management - move items between storage accounts and containers
- Data access - access and manipulate blob items
- SDKs
- Retention and data lifecycle / archiving options
- Use of Storage Explorer, Azure CLI, Azure Portal for all of the above

#### Metafata

Metadata is key-value pairs to store important information about the data
    - Content type
    - Language
    - Other

#### Using the .NET SDK to maniuplate storage data

- Read/write/update metadata

#### Azure Blob Storage data lifecycle

Lifecicle Management used to manage data lifecycle:

Creation > Frequent Access > Infrequent Access > Archiving/deletion

- **Hot Access**
  - Optimized for frequent access
  - More expensive
- **Cool Access**
  - Optimized for less frequent access
  - **Data that stored for at least 30 days**
  - Cheaper
- **Archive**
  - Data no longer needs to be accessed
  - Set retention policies based on time

AZ-203 used to include table storage & relational database but they are not included in AZ-204.

## Implement Azure security - 20-25%

4 Azure security feature groups:

- **Transparency** - a high degree of transparency on how data is managed and secured so that you can be confident that your data is protected
- **Compliance** - many offerings in Azure helping you meet compliance and regulatory requirements
- **Data Access** - might need to be controlled by region, regulations, or withing security groups or time constraints
- **Monitoring** - Azure services provide many monitoring & alerting options

AZ 204 exam security OD includes only 2 topics:

- Authentication and authorization
- Access Azure Resources

### Authentication and authorization

- Authentication - validating user's identities
- Authorization - giving access to those identities (validating identity permissions for specific operation)

### Azure AD (AAD)

- AAD handles both authentication and authorization
- Security principals (people and applications identies)
  - User principals
  - Service principals
  - Managed identities (tie security principal to app, removing need for storing and managing credentials)

- Azure Key Vault - secrets storage which can store
  - Keys
  - Secrets
  - Certificates

#### Open Authorization Framework (OAuth)

- Openly available authorization pattern

- **Traditional authorization** - restriction and access revokation difficulties
  - User credentials - username and password
- **OAuth2**
  - Provide an access token which contains
    - Scope
    - **Lifetime**
  - Acquire tokens using the MSFT Identity platform using
    - .NET SDK
    - Azure CLI

#### Shared Access Signature (SAS) tokens

- Provide delegated access to resources (usually storage accounts)
- Used instead of providing direct access to storage account
- Access options
  - **Connection string**
    - Unrestricted access
    - Revoked by invalidating key
  - **SAS token**
    - Define operations
    - Provide scope
    - Provide time limit

#### Azure Active Directory (AAD)

- Azure framework for authentication and authorization
- User authentication
  - Register AAD application
  - Add users
- **RBAC**
  - Assign users or applications a role
  - Access rules defined by role assigned to user/application
  - Azure predifined RBAC roles
    - Owner - full access and permission management
    - Read and write access, no permissions management
    - Reader - read-only access
  - RBAC roles can be applied on different levels
    - Resource
    - Resource Group
    - Subscription
  - Service specific RBAC roles
    - Data blob contributor
    - etc.
  - Custom RBAC roles
    - Define your own access rulesets
  - Create and assign access roles using
    - Azure Portal
    - Azure CLI

#### Access Azure Resources

- Azure App Configuration
- Azure Key Vault
- Managed Identities

### Implement secure Azure Solutions

#### Azure Key Vault

- Secure keys, certificates, and secrets - centralized store and usage monitoring
  - Keys - generate or bring your own
    - Type - RSA / EC
    - RSA Key Size - 2048, 3072, 4069
    - Activation/Expirtation date
  - Secrets
    - Any values that need to be stored securely (passwords, connection strings) - values are versioned with previous versions visible
  - Certificates (TLS/SSL)
- Storing app config using API
- Using CLI and .NET SDK, manage:
  - Keys
  - Secrets
  - Certificates
- How to create and set KV secrets using KV URI & .NET SDK
- Azure app configuration - centralizing and managing app settings
- Managed identities - to remove the need to store secrets

- Identities
  - **Service principals**
    - Provide identities to applications
    - Store credentials
  - **Managed identities**
    - Provide identities to applications
    - Linked to applications
    - No credential management
    - Use to access - Azure Functions, VMs, Azure App Service

## Monitor, troubleshoot, and optimize Azure solutions - 15-20%

### Content Delivery and caching

- **Caching** - storing local copy of data so it doesn't need to be re-retrieved from the server/remote location each time it's accessed/storing data on a fast/closer storage
- Caching requires invalidation mechanism on master data change
- Caching can also be helpfull to offload multiple/heavy concurrent reads from your primary source
- Implementing a content delivery network
- Caching policies

- [Caching types and best practices](https://docs.microsoft.com/en-us/azure/architecture/best-practices/caching)
  - **Private caching** - one persistent data storage, aplication instances retrieve data and then store it in its in-memory cache (e.g. SQL server and multiple insances of app); as **each instance has its own in-memory cache** this approach is called private caching
  - **Shared caching** - one persistent data storage with **shared service cache in front** of it used by multiple application instances
  - [Cache-Aside pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/cache-aside) - Load data on demand into a cache from a data store. This can improve performance and also helps to maintain consistency between data held in the cache and data in the underlying data store
    - Determine if item is currently held in cache
    - If the item is not currently in the cache, read the item from the data store
    - Store a copy of the item in the cache
- Caching is efficient/viable when
  - Data is relatively static
  - If you have a large number of clients reading data at the same time and causing contention (number of concurrent clients limits)
- Consideration for using caching
  - Decide when to cache data
  - Determine how to cache data effectively
  - Cache highly dynamic data (sensors data)
  - Manage data expiration in a cache (**eviction policies** & **data expiration settings**)

#### Content delivery networks (CDN)

- CDN - efficiently delivers web content to users via caching; improves performance delivering content through servers which are physically closer to consumers (point of presence locations)
- CDN profiles & CDN endpoints set up to serve content
- CDN profile
  - CDN Profile name, subscription, RG
  - Pricing
  - CDN endpoint name
  - Origin type
    - Storage
    - Storage static website
    - Cloud service
    - Web app
    - Custom origin

#### Azure Front Door caching and expiration policies

- **Azure Front Door** - CDN implementation which provides a secure single single point of entry to applications, APIs & content
  - Control web traffic
  - Monitor traffic
  - Implement caching policies (caching headers & rules)

#### Azure Cache for Redis

- In-memory content store/cache
- Provides a session state provider that you can use **to store your session state in-memory**
- Connect to Azure Cache for Redis
- Managed-service based on open-source Redis caching software - in-memory DB/cache/message broker
- When to use Azure Cache for Redis
  - When you need managed service compatible with Redis
  - When you need HA and automatic scaling
- Azure cache for Redis tiers
  - Basic
    - based on open-source Redis
  - Standard
    - based on open-source Redis
    - runs on 2 replicated VMs
  - Premium
    - higher performance & throughput, lower latency
    - runs on more powerful VMs
  - Enterprise
    - based on commercial implementation from Redis Labs
    - provides dual replication
    - provides advanced features - Redis Bloom, RediSearch, RedisTimeSeries
  - Enterprise Flash
    - based on commercial implementation from Redis Labs
    - extens in-memory storage over to a non-volatile data storage for cost efficiency (non-volatile storage is cheaper than DRAM)
- Creating an Azure Cache instance
  - maxclients - 256
  - maxfragmentationmemory-reserved - 12
  - SSL port - 6380
  - Minimum TLS level - TLS 1.0 & 1.1 bound to be retired
  - **maxmemory-policy** - data eviction policy
- [Scaling Azure Cache for Redis instance](https://azure.microsoft.com/en-us/pricing/details/cache/)
  - Basic - Simplified cache ideal for development/testing
  - Standard - Production ready cache with primary/replica duplication
  - Premium - Advanced enterprise features like clustering and data persistence
  - Enterprise - All the functionality of the Premium tier plus powerful enterprise-ready features like Redis Modules and active geo-replication
  - Enterprise Flash - All the functionality of the  Enterprise tier, but running on fast non-volatile storage for massive, yet cost-effective, cache implementations

| Feature                         | Basic          | Standard       | Premium        | Enterprise       | Enterprise Flash |
|---------------------------------|----------------|----------------|----------------|------------------|------------------|
| **Memory size**                 | 250 MB - 53 GB | 250 MB - 53 GB | 6 GB-120 GB    | 12 GB-100 GB     | 384 GB-1.5 TB    |
| **Availability**                | n/a            | 99,9%          | Up to 99,9%    | Up to 99,999%    | Up to 99,999%    |
| Azure Private Link              | Yes            | Yes            | Yes            | Yes              | Yes              |
| Replication & Failover          | n/a            | Yes            | Yes            | Yes              | Yes              |
| Zone redundancy                 | n/a            | n/a            | Yes            | Yes              | Yes              |
| Redis data persistance          | n/a            | n/a            | Yes            | Yes              | Yes              |
| Redis cluster                   | n/a            | n/a            | Yes            | Yes              | Yes              |
| **Virtual Network**             | n/a            | n/a            | Yes            | n/a              | n/a              |
| Geo Replication                 | n/a            | n/a            | Passive        | Active           | Active           |
| RediSearch                      | n/a            | n/a            | n/a            | Yes              | n/a              |
| RedisBloom                      | n/a            | n/a            | n/a            | Yes              | n/a              |
| RedisTimeSeries                 | n/a            | n/a            | n/a            | Yes              | n/a              |
| Redis ob Flash                  | n/a            | n/a            | n/a            | n/a              | Yes              |
| **Max N of client connections** | 256 - 20,000   | 256 - 20,000   | 7,500 - 40,000 | 50,000 - 200,000 | 50,000 - 120,000 |

- Within the tiers from the table above there is a sizing CN/PN which will define cache size, number of client connections and pricing - the higher the N the higer everytrhing
- Some tier/size specs:
  - C0 Basic - 250 MB cache, low NW bandwidth, shared infra, SSL up to 256 connections
  - C0 Standard - 250 MB cache, low NW bandwidth, shared infra, SSL up to 256 connections + 99.9 SLA
  - C1 Basic - 1 GB cache, low NW bandwidth, dedicated service, SSL, up to 1000 connections
  - C2 Basic - 2,5 GB cache, moderate NW bandwidth, dedicated service, SSL, up to 2000 connections
  - C3 Basic - 6 GB cache, moderate NW bandwidth, dedicated service, SSL, up to 5000 connections
  - C4 Basic - 13 GB cache, up to 10,000 connections
  - C5 Basic - 26 GB cache, up to 15,000 connections
  - C6 Basic - 53 GB cache, up to 20,000 connections
  - P1 Premium - 6 GB cache, replication
  - P2 Premium - 13 GB cache,replication
- Premium/enterprise tiers allow
  - Control cluster size
  - Control data persistence
  - VNET injection

- Azure Cache for Redis persistence models
  - **Append only file (AOF)** - With AOF persistence, every write operation is saved to a log. The log is saved at least once per second into an Azure Storage account.
  - **Redis database (RDB)** - When you use RDB persistence, Azure Cache for Redis snapshot is persisted in an Azure Storage account based on configurable backup frequency.

```PowerShell
az group create --name AZ204RG --location eastus
az redis create --name AZ204redis --resource-group AZ204RG --location eastus --sku Basic --vm-size C0
```

```C#
// Connecting to Azure Cache for Redis using ConnectionMultiplexer class
string cacheConnection = ConfigurationManager.AppSettings["CacheConnection"].ToString();
return ConnectionMultiplexer.Connect(cacheConnection);
```

```C#
// Set and retrieve data from Redis Cache - StringSet & String Get
IDatabase cache = Connection.GetDatabase();
cache.StringSet("Value1", "A value");
string value1 = cache.StringGet("Value1")
Console.Writeline(value1);
// To invalidate cache value use cache.KeyDelete
```

- [Quickstart: Use Azure Cache for Redis with an ASP.NET Core web app](https://docs.microsoft.com/en-us/azure/azure-cache-for-redis/cache-web-app-aspnet-core-howto)
  - Download ContosoTeamStats app code
  - Edit code in VS - inside of HomeController.cs review interaction with cache
  - Before running setup connection string: appsettings.json > **"CacheConnection":** - put connection string value from Azure Cache for Redis Access Keys from the Portal, add **allowAdmin=true** as it is required for client list operation (using StackExchange.Redis)
  - Client library - **using StackExchange.Redis** - allows to execute commands against Redis cache
  - Lazy load to build connection - **lazyConnection**

```C#
// use of client library, ConnectionMultiplexer and Lazy Load to connect to Redis Cache
using StackExchange.Redis
private static Lazy<ConnectionMultiplexer> lazyConnection = CreateConnection();

public static ConnectionMultiplexer Connection
{
    get
    {
        return lazyConnection.Value;
    }
}
```

#### [Selecting Redis (pricing) tiers](https://azure.microsoft.com/en-us/pricing/details/cache/#pricing)

- **Requirements set #1**
  - Web application design needs to include a caching tier = Use Redis
  - Memory size 50 GB = Basic/Standard is OK (250MB-53GB)
  - Max number of client connections 10,000 = Basic/Standard is OK (support 256 - 20,000 connections)
  - **Availability SLA 99,9%** = **we need Standard or higher tier for this**
- **Requirements set #2**
  - Web application design needs to include a caching tier = Use Redis
  - Memory size 50 GB = Basic/Standard is OK (250MB-53GB)
  - Max number of client connections 20,000 = Basic/Standard is OK (support 256 - 20,000 connections)
  - **Availability SLA 99,9%** = **we need Standard or higher tier for this**
  - **Virtual network connectivity** = **Premium tier (the only tier that supports this)**

#### Eviction Policies

- Define what action to take when Redis cache reaches its memory limits
- Volatile keys - keys with defined expiration time

```Bash
# https://redis.io/commands/set
# setting key without expiration, until explicit delete
redis> SET mykey "Hello"
"OK"
redis> GET mykey
"Hello"
# setting key with predefined expirtation time
redis> SET anotherkey "will expire in a minute" EX 60
"OK"
redis>
```

- **Eviction policies**
  - noeviction - returns an error if the memory limit has been reached when trying to insert more data
  - allkeys-lru - evicts the least recently used keys out of all keys
  - allkeys-lfu - evicts the least frequently used keys out of all keys
  - allkeys-random - randomly evicts keys out of all keys
  - volatile-lru - evicts the least recently used keys out of all keys with expire field set
  - volatile-lfu - evicts the least frequently used keys out of all keys with expire field set
  - volatile-random - randomly evicts keys with an expire field set

- Selecting eviction policy
  - Some cache keys never seem to expire - change policy from volatile to allkeys-lru
  - Some frequently used cache keys seem to expire - change policy to volatile-lfu

### Monitoring Azure Resources

Goal of monitoring is diagnosting faults and maximizing service up-time.

- Configure Application Insights
- Implement web test and alerts
- Use Azure Monitor to analyze and diagnose failures
- Protect against transient failures (writing code which handles those)

#### Application Insights

- Service for monitoring Azure services, part of Azure Monitor (which in turn provides specific logging solutions for Azure Storage, Cosmos DB, VMs, and containers)
  - Logs
  - Metrics
- We provide service with **Application Insights Instrumentation Key** & configure it to send logging to App Insights instance
- Once configured Application Insights instance allows to visualise/analyze collected metrics (failed requests, CPU usage, etc.) & alerts or use transaction search
- We can set/trigger alerts based on
  - Metrics (error or data volume growth, CPU usage growth etc.)
  - Failures
  - Custom logs
- Usage of app insights: Add App insights instrumentation key to Azure Service > Configure Azure Service to send logging to Application insights (metrics, log telemetry)

#### Azure Monitor

- Provides specific logging solutions for Azure Storage, Cosmos DB, VMs, and containers
  - Tracking
  - Troubleshooting
- Dasboards and workbooks can be used to create visualisations based on log data
- Smart alerts can be set up to dynamically monitor for anomalies and failures
- Azure Logic Apps have alert integration & allow to send email or SMS based on alert
- Azure Monitor Logging Query

```KQL
requests
| where resultCode == "500"
```

- **Azure Log Analytics** - used to view, query, and analyze logs from Azure Monitor

#### Transient failure (handling with defensive code)

- A **transient failure** is an error caused by **temporarily** unavailable/failing service = retry after delay should succeed
- Often such errors are handled by SDKs and tools but sometimes you need to write your code to handle them too
- Write defensive code with retry logic/policies

## Connect to and consume Azure services and third-party services 15-20%

### Azure Logic Apps

- Azure Logic apps = automated workflows/orchestrations
- Azure Logic Apps is a service which allows to build workflows/work with tasks
  - Schedule
  - Automate
  - Control
  - Orchestrate
- Allows to visually built up the steps
- Includes multitude of built-in connectors
- Workflow control can include control flow logic - conditions (IF FAIL etc.), switch statements, loops and branching
- Workflows can be invoked via triggers (time, condition, event)
- Common triggers
  - When a message is received in a Service Bus queue
  - When a HTTP request is received
  - When a new tweet is posted
  - When an Event Grid resource event occurs
  - Recurrence
  - When a new email is received in Outlook.com
  - When a new file is created on OneDrive
  - When a file is added to FTP server
  - When a blob is added to storage account
- Create custom connectors - wrapper arround REST API endpoint
    1. Create and secure endpoint (Azure Functions, web app, API app)
    2. Define API (Open API, swagger)
    3. Add custom connector template
    4. Add custom connector to workflow
- Create templates

### Azure APIM

- APIM - service to create consistent and secure gateways for your API endpoints providing:
  - Security
  - Quotas
  - Caching
  - Diagnostics
- Used to control and manage traffic, more specifically to
  - Route traffic
  - Verify access
  - Enforce quotas and limits
  - Transform (stip out response headers, change response URLs)
  - Control caching
  - Improve logging (logging call metadata)
- Create a new APIM instance
- Configure authentication policies
  - Basic authentication - username and password
  - Client certificate - a certificate installed on the APIM instance
  - Managed identity - using services' built-in identities
- Configure policies
  - **Rate limit** - Rate-limited APIs will only handle a certain amount of calls in a certain amount of **seconds**
    - Rate-Limited

        ```xml
        <rate-limit calls="20" renewal-period="60" />
        ```

        = 20 calls every minute per subscription
    - Renewal policy (renewal-period) is in seconds, and max value of it in rate policy is 300 seconds (5 mins)
    - Rate-Limited by key <rate-limit-by-kye calls="20" renewal-period="60" counter-key="@(context.Request.IpAddress)" />
  - **Usage Quota** = limit to a certain number of calls in a time period, can be scoped to subscription or to a key
    - Quota by key <quota-by-key calls="100" bandwidth="40000" renewal-period="3600" counter-key="@(context.Request.IpAddress" />
      - bandwidth in KB
      - renewal period in seconds, 3600 is a min value (60 min)
  - Rate VS Quota
    - **Renewal period more than 300 seconds (5 min)** = Usage quota
    - **Prevent usage spikes** = Rate limited

### Event-based solutions

- Producers publish events to a stream for receivers to subscribe to
  - Producers publish events to a stream
  - Consumers subscribe
  - Stream can have multiple subscribers
  - Producer has no control over subscribers (services are completely decoupled)
  - Use case: **decoupling systems**
- 3 Azure services for event-driven architectures:
  - Azure Event Grid
  - Azure Event Hubs
  - Azyre Notification Hubs

#### Azure Event Grid

- Service for connecting data sources and handlers
- Source sends events to topic end point
- Topics are used for collection of related events
- You can use built-in topics or create custom ones
- Many Azure service have support to act as an Event Grid topics
  - Blob Storage
  - Container Registry
  - IoT Hub
  - Key Vault
  - Azure ML
  - etc.
- Event sources - we capture events occurring on the specified source/scope
  - RGs
  - Subscriptions
- Consumers
  - Function Apps
  - Logic Apps
  - Azure key storage
  - Event Hubs (event ingestion service)

#### Azure Event Hubs

- An event ingestion service which can process large volumes of messages
- Use cases: IoT, Analytics, Log streaming
- **Event Producer** - sends events into the hub
- **Namespace** - a collection of event hubs
- Set number of **throughput units** per namespace, on exceeding Service Busy exception is returned
- **Partitions** - events are organized into partitions within a hub
- Newly arrived event added to the end of the partition
- Ordering is preserved/guaranteed within a single partition
- **Retention time** - the length of time that events are stored for (max for a Standard EH = 7 days), after this time events are deleted
- **Event Receiver** - anything that reads events from the hub

#### Azure Notifications Hubs

- Allow you to send **mobile push notifications**
  - iOS
  - Android
  - Windows
  - Kindle
- Events can be broadcasted to a large number of devices simultaneously with low latency

### Message-based systems

- Producers send messages to specific receivers/consumers
- E.g. storage account sending event (new file added etc.) to Azure Function

#### Azure Service Bus

- Enterprise-scale message broker
  - Queues
    - When messages arrive they are time stamped and added to the queue in the order of arrival
    - Receiver consumes messages from the queue and removes it from the queue once it is processed
    - Messages are pulled from the queue by consumer/receiver
    - Pull operations can be long running
  - Topics
    - Can have multiple queues and copies of the messages
    - Can be subscribed by multiple receivers
    - Looks the sambe from subscriber POV but allows you to have multiple subscribers
    - Emulates event driven architecture with enterprise level consistency
- Use Service bus when you need
  - To include data in messages
  - Enterprise-scale auditing
  - Consistency
  - Ordering
  - Transactional processing

#### Azure Storage Queues

- Used to store high volumes of messages
- Can store up to storage account capacity (5 Pebibytes)
- max 65 KB per message
- 7 days - default message retention, can be set to indefinite time
- Storage queues are usually used to create a backlog of work that can be processed asynchronously
- Can integrate with other Azure services - Azure Functions, Web Apps, Event Grid, etc.

#### Select event-based solution based on requirements

- Stream processing of large volumes of events = Azure Event Hubs
- Ordering across partitions - Azure Service Bus
- Azure service bus message storing limit = 80 GB, if you need more = Azure Storage Queues
- Adding a message to the queue

    ```C#
    QueueClient queue = new QueueClient(connectionString, "a-queue");
    await theQueue.SendMessageAsync("A new message!");
    ```

- Retrieve a message from the queue

    ```C#
    // retrieve up to 5 messages within 1 minute
    QueueMessage[] retrievedMessages = await queue.ReceiveMessagesAsync(5, new TimeSpan(0, 1, 0));
    Foreach (QueueMessage message in retrievedMessages)
    {
        await queue.DeleteMessageAsync(message.MessageId, message.PopReceipt);
    }
    ```

### Automation Tools

- Command-line and SDK knowledge - exam focuse on Azure CLI & .NET SDK
- Automation Tools
  - Azure CLI
  - .NET SDK
  - Python SDK
  - ARM template syntax

- NO NEED to recognize exact syntax
- NEED to know outline and ordering

#### Azure CLI

- Cross-platform command-line interface for automating resource management
- Can be installed/run on
  - Windows
  - Mac
  - Linux
  - Docker
  - Browser (within cloud shell)
- Supports
  - Long running operations
  - Cross-subscription resource management

```
az group create --name demogroup --location northeurope
az storage account create --name demostorage --resource-group demogroup
az storage container create --account-name demostorage --name container1
az storage blob upload --account-name deomstorage --container-name container1 --name blob1
```

#### .NET SDK

- Used to manage and access resources using C#
- Installation - NuGet package manager
- Adds required classes and name spaces

```C#
// Working with message queue example
namespace AZ_204_Demo
{
    class Program
    {
        static void Main(string[] args)
        {
            string connectionString = ConfigurationManager.AppSettings["StorageConnectionString"];
            QueueClient queue = new QueueClient(connectionString, "queue");
            queue.CreateIfNotExists();
            queue.SendMessage("Test message!");
            var messages = queue.ReceiveMessages(5, TimeSpan.FromSeconds(60));
            foreach(var message in messages.Value)
            {
                Console.WriteLine(message.Body);
                queue.DeleteMessage(message.MessageId, message.PopReceipt);
            }
        }
    }
}
```

#### Pyton SDK

- Used to manage resources using Python

#### ARM Template

- A JSON-like language used for managing resources

### Choosing the appropriate Azure resources

#### Question type: Grouping and selecting Azure Resources

- Set of requiremenents (potentially split into sections - security, resource specific)
- Does the proposed solution met set of requirements
- Example
  - Requirements
    - Secure storage account access
    - Ability to allow users access for a certain amount of time
  - Solutions
    - 1 - Put the account connection string into Key Vault key and give users permissions to read the key - NO - KV key only can store encryption keys, can't store connection string
    - 2 - Put the account connection string into Key Vault secret and give users permissions to read the secret - NO - Connection string can be stored as a secret but it won't allow granting temorary access, we can only revoke it through regeneration of account keys
    - 3 - Generate SAS tokens and give them to users - OK - expiration time can be set for SAS tokens

##### Resource Groupings #####

- Azure Compute
  - Azure Functions
  - Azure Containers
  - Azure VMs
- Infrastructure management grouping
  - Azure Functions - All managed for you / Requirement - Spend as little time on infra management as possible
  - Azure Containers - Lightweight + control over OS / Requirement - Lightweight, predictable environments with no HW management
    - Lightweight
    - Repeatable
    - No control over hardware
    - Control over OS
  - Azure VMs - Requirement: Complete control over infra and networking
    - Control over machines
    - Control over infrastructure

- Azure Storage
  - Azure Blob Storage
  - Azure Cosmos DB
- Pricing grouping
  - Azure Blob Storage - Requirement- Storing terabytes of data (at the lowest cost)
    - Large amounts of data
    - Low cost
  - Azure Cosmos DB - Requirement - **Complex queries over small data volumes**
    - Smaller data quantities
    - Complex queries

- Security with Azure RBAC grouping
  - Reader - Requirement - Read-only resource access
  - Contributor - Requirement - Write access to resources
  - Owner - Requirement - Permissions management

- Monitoring and optimization
  - Azure Front Door - Requirement - Serving entire web services
  - Azure CDN - Requirement - Serving static content

- Event and Message-Based Solutions (Integration)
  - Azure Event Grid - Requirement - Multiple consumers reading to react to events that happened
    - Reactive processing
    - Event distribution
    - **Lightweight notifications**
  - Azure Event Hubs - Requirement - Big data processing with order preserved in a single partition
    - Streaming events (in series)
    - Ordering preserved within a partition
  - Azure Service Bus - Requirement - High consistency and < 80 GB concurrent data storage
    - Enterprise scale audit and ordering
    - 80 GB total storage
  - Azure Storage Queues - Requirement - High message volumes (> 80 GB)
    - High total storage limits
    - 65kB/message

##### Key Differentiators/Considerations #####

- Price
  - Low cost solutions
- Infrastructure (level of control)
  - Level of control over infra and connections
  - Ruling out fully management offering
  - Replication and predictable redeployment of infra
  - Scaling up/out
- Security
  - Amount of fine-grained access control
  - Ability to revoke access
  - Requirements around not storing credentials
- Reliability
  - Guarantees of not losing any information
  - Events ordering
  - Visibility of events within the system (trace logs)
  - Cheap high volume backups
  - Backups replication across different DC
- Type of data/access
  - Data type - relational, unstructured?
  - Complex queries?
  - Multiple consumers?
  - High levels of consistency?
  - Quick content availability worldwide?
- Hard limits
  - Message size > 65kb = Azure Queue can't be used
  - Total messages > 80GB = Azure Service Bus can't be used
  - Renewal time > 5 mins = Rate limited APIM access policy can't be used (use quota policy)
